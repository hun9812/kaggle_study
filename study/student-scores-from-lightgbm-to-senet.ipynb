{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":119082,"databundleVersionId":14993753,"sourceType":"competition"},{"sourceId":13904981,"sourceType":"datasetVersion","datasetId":8762382}],"dockerImageVersionId":31234,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"V1-V22 uses the lightgbm model, with the best version being V11.\n\nFrom V23， try other models","metadata":{}},{"cell_type":"markdown","source":"reference: https://www.kaggle.com/jiaoyouzhang","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import OrdinalEncoder, StandardScaler\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import TensorDataset, DataLoader","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T13:28:28.370289Z","iopub.execute_input":"2026-01-30T13:28:28.370550Z","iopub.status.idle":"2026-01-30T13:28:35.104684Z","shell.execute_reply.started":"2026-01-30T13:28:28.370528Z","shell.execute_reply":"2026-01-30T13:28:35.103709Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"OrdialEncoder: 문자(카테고리)데이터를 숫자로 ('S','M' -> 0,1)  \nStandardScalar: 데이터 단위 통일 (평균 0, 분산 1)\n\ntorch: PyTorch  \ntorch.nn as nn: 신경망의 layer와 구조를 만듬  \noptim: 최적화 도구 (Adam, SGD)  \nTensorDataset: 입력 데이터와 정답을 하나로 묶어줌  \nDataLoader: 묶인 데이터를 학습하기 좋게 batch  ","metadata":{}},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\ntrain_file = \"/kaggle/input/playground-series-s6e1/train.csv\"\ntest_file = \"/kaggle/input/playground-series-s6e1/test.csv\"\noriginal_file = \"/kaggle/input/exam-score-prediction-dataset/Exam_Score_Prediction.csv\"\n\ntrain_df = pd.read_csv(train_file)\ntest_df = pd.read_csv(test_file)\noriginal_df = pd.read_csv(original_file) \nsubmission_df = pd.read_csv(\"/kaggle/input/playground-series-s6e1/sample_submission.csv\") \nTARGET = 'exam_score'\n\nnum_features = ['study_hours', 'class_attendance', 'sleep_hours']\nbase_features = [col for col in train_df.columns if col not in [TARGET, 'id']]\nCATS = base_features\nNUMS = num_features  # only these are truly numerical","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T13:28:35.107101Z","iopub.execute_input":"2026-01-30T13:28:35.107573Z","iopub.status.idle":"2026-01-30T13:28:36.332723Z","shell.execute_reply.started":"2026-01-30T13:28:35.107544Z","shell.execute_reply":"2026-01-30T13:28:36.331784Z"}},"outputs":[{"name":"stdout","text":"Using device: cpu\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"현재 컴퓨터에 CUDA가 잇는지 확인하고 있으면 CUDA, 없으면 CPU 선택  \n\ntrian, test, original file을 가져옴\n\nTARGET변수 목표를 넣고,  \nnum_features에 숫자 데이터  \nbase_features에 모든 컬럼 (target, id 제외)  \n\nCATS: 범주형  \nNUMS: 숫자형  ","metadata":{}},{"cell_type":"code","source":"def add_engineered_features(df):\n    df_temp = df.copy()\n    # Sine features\n    df_temp['_study_hours_sin'] = np.sin(2 * np.pi * df_temp['study_hours'] / 12).astype('float32')\n    df_temp['_class_attendance_sin'] = np.sin(2 * np.pi * df_temp['class_attendance'] / 12).astype('float32')\n\n    for col in num_features:\n        if col in df_temp.columns:\n            df_temp[f'log_{col}'] = np.log1p(df_temp[col])\n            df_temp[f'{col}_sq'] = df_temp[col] ** 2\n\n    for col in train_df.select_dtypes(include=['object','category']).columns.tolist():\n        cat_series = df_temp[col].astype(str)\n        freq_map = cat_series.value_counts().to_dict()\n        df_temp[f\"{col}_freq\"] = cat_series.map(freq_map).fillna(0).astype(int)\n        \n    # Linear combo feature\n    df_temp['feature_formula'] = (\n            5.9051154511950499 * df_temp['study_hours'] +\n            0.34540967058057986 * df_temp['class_attendance'] +\n            1.423461171860262 * df_temp['sleep_hours'] + 4.7819\n    )\n\n    # Keep categorical as string for encoding\n    for col in CATS:\n        df_temp[col] = df_temp[col].astype(str)\n\n    return df_temp","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T13:28:36.333567Z","iopub.execute_input":"2026-01-30T13:28:36.333775Z","iopub.status.idle":"2026-01-30T13:28:36.340433Z","shell.execute_reply.started":"2026-01-30T13:28:36.333756Z","shell.execute_reply":"2026-01-30T13:28:36.339544Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"새로운 데이터 만드는 Feature Engineering  \n\n_study_hour_sin이라는 새로운 컬럼을 만들음 \n공부를 하는 시간에 다른 패턴이 있나? (0-12시간을 한 주기로 보고)  \n실험적인 의도라고 생각함  \n\nfor col in num_features: 수치형 데이터 변환(로그 & 제곱)  \n수치형 데이터는 각각 log와 제곱 컬럼을 추가  \n\nfor col in train_df.select_dtypes  \n원본 학습 데이터(trian_df)에서 글자(object)나 범주(cat)인 컬럼  \ncat_series => 해당 열을 확실하게 문자열로  \nfreq_map => 해당 열에서 각 항목이 몇번 나왔는지 dict로 변환  \n{col}_freq 컬럼에 각 컬럼의 빈도수를 숫자로 바꿔서 저장, 빈칸은 0으로 채우고 null로 변환  \nex) A학교 -> 50, B학교 -> 10, A학교 -> 50  \n\nfeature_formula: 작성자가 이 모델을 만들기 전에 linear regression같은  \n단순한 모델을 먼저 돌려엇 얻은 최적의 계수  \n\nfor col in CATS:  \n범주형 데이터를 문자열로 고정하고 return  ","metadata":{}},{"cell_type":"code","source":"train_eng = add_engineered_features(train_df)\n\nall_num_cols = [col for col in train_eng.columns if col not in CATS + [TARGET, 'id']]\nall_cat_cols = CATS\n\nscaler = StandardScaler()\nscaler.fit(train_eng[all_num_cols])\n\nencoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\nencoder.fit(train_eng[all_cat_cols])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T13:28:36.341348Z","iopub.execute_input":"2026-01-30T13:28:36.341600Z","iopub.status.idle":"2026-01-30T13:28:38.180540Z","shell.execute_reply.started":"2026-01-30T13:28:36.341580Z","shell.execute_reply":"2026-01-30T13:28:38.179261Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)","text/html":"<style>#sk-container-id-1 {\n  /* Definition of color scheme common for light and dark mode */\n  --sklearn-color-text: #000;\n  --sklearn-color-text-muted: #666;\n  --sklearn-color-line: gray;\n  /* Definition of color scheme for unfitted estimators */\n  --sklearn-color-unfitted-level-0: #fff5e6;\n  --sklearn-color-unfitted-level-1: #f6e4d2;\n  --sklearn-color-unfitted-level-2: #ffe0b3;\n  --sklearn-color-unfitted-level-3: chocolate;\n  /* Definition of color scheme for fitted estimators */\n  --sklearn-color-fitted-level-0: #f0f8ff;\n  --sklearn-color-fitted-level-1: #d4ebff;\n  --sklearn-color-fitted-level-2: #b3dbfd;\n  --sklearn-color-fitted-level-3: cornflowerblue;\n\n  /* Specific color for light theme */\n  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n  --sklearn-color-icon: #696969;\n\n  @media (prefers-color-scheme: dark) {\n    /* Redefinition of color scheme for dark theme */\n    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n    --sklearn-color-icon: #878787;\n  }\n}\n\n#sk-container-id-1 {\n  color: var(--sklearn-color-text);\n}\n\n#sk-container-id-1 pre {\n  padding: 0;\n}\n\n#sk-container-id-1 input.sk-hidden--visually {\n  border: 0;\n  clip: rect(1px 1px 1px 1px);\n  clip: rect(1px, 1px, 1px, 1px);\n  height: 1px;\n  margin: -1px;\n  overflow: hidden;\n  padding: 0;\n  position: absolute;\n  width: 1px;\n}\n\n#sk-container-id-1 div.sk-dashed-wrapped {\n  border: 1px dashed var(--sklearn-color-line);\n  margin: 0 0.4em 0.5em 0.4em;\n  box-sizing: border-box;\n  padding-bottom: 0.4em;\n  background-color: var(--sklearn-color-background);\n}\n\n#sk-container-id-1 div.sk-container {\n  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n     but bootstrap.min.css set `[hidden] { display: none !important; }`\n     so we also need the `!important` here to be able to override the\n     default hidden behavior on the sphinx rendered scikit-learn.org.\n     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n  display: inline-block !important;\n  position: relative;\n}\n\n#sk-container-id-1 div.sk-text-repr-fallback {\n  display: none;\n}\n\ndiv.sk-parallel-item,\ndiv.sk-serial,\ndiv.sk-item {\n  /* draw centered vertical line to link estimators */\n  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n  background-size: 2px 100%;\n  background-repeat: no-repeat;\n  background-position: center center;\n}\n\n/* Parallel-specific style estimator block */\n\n#sk-container-id-1 div.sk-parallel-item::after {\n  content: \"\";\n  width: 100%;\n  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n  flex-grow: 1;\n}\n\n#sk-container-id-1 div.sk-parallel {\n  display: flex;\n  align-items: stretch;\n  justify-content: center;\n  background-color: var(--sklearn-color-background);\n  position: relative;\n}\n\n#sk-container-id-1 div.sk-parallel-item {\n  display: flex;\n  flex-direction: column;\n}\n\n#sk-container-id-1 div.sk-parallel-item:first-child::after {\n  align-self: flex-end;\n  width: 50%;\n}\n\n#sk-container-id-1 div.sk-parallel-item:last-child::after {\n  align-self: flex-start;\n  width: 50%;\n}\n\n#sk-container-id-1 div.sk-parallel-item:only-child::after {\n  width: 0;\n}\n\n/* Serial-specific style estimator block */\n\n#sk-container-id-1 div.sk-serial {\n  display: flex;\n  flex-direction: column;\n  align-items: center;\n  background-color: var(--sklearn-color-background);\n  padding-right: 1em;\n  padding-left: 1em;\n}\n\n\n/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\nclickable and can be expanded/collapsed.\n- Pipeline and ColumnTransformer use this feature and define the default style\n- Estimators will overwrite some part of the style using the `sk-estimator` class\n*/\n\n/* Pipeline and ColumnTransformer style (default) */\n\n#sk-container-id-1 div.sk-toggleable {\n  /* Default theme specific background. It is overwritten whether we have a\n  specific estimator or a Pipeline/ColumnTransformer */\n  background-color: var(--sklearn-color-background);\n}\n\n/* Toggleable label */\n#sk-container-id-1 label.sk-toggleable__label {\n  cursor: pointer;\n  display: flex;\n  width: 100%;\n  margin-bottom: 0;\n  padding: 0.5em;\n  box-sizing: border-box;\n  text-align: center;\n  align-items: start;\n  justify-content: space-between;\n  gap: 0.5em;\n}\n\n#sk-container-id-1 label.sk-toggleable__label .caption {\n  font-size: 0.6rem;\n  font-weight: lighter;\n  color: var(--sklearn-color-text-muted);\n}\n\n#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n  /* Arrow on the left of the label */\n  content: \"▸\";\n  float: left;\n  margin-right: 0.25em;\n  color: var(--sklearn-color-icon);\n}\n\n#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n  color: var(--sklearn-color-text);\n}\n\n/* Toggleable content - dropdown */\n\n#sk-container-id-1 div.sk-toggleable__content {\n  max-height: 0;\n  max-width: 0;\n  overflow: hidden;\n  text-align: left;\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-1 div.sk-toggleable__content.fitted {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n#sk-container-id-1 div.sk-toggleable__content pre {\n  margin: 0.2em;\n  border-radius: 0.25em;\n  color: var(--sklearn-color-text);\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n  /* unfitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n  /* Expand drop-down */\n  max-height: 200px;\n  max-width: 100%;\n  overflow: auto;\n}\n\n#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n  content: \"▾\";\n}\n\n/* Pipeline/ColumnTransformer-specific style */\n\n#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Estimator-specific style */\n\n/* Colorize estimator box */\n#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n#sk-container-id-1 div.sk-label label {\n  /* The background is the default theme color */\n  color: var(--sklearn-color-text-on-default-background);\n}\n\n/* On hover, darken the color of the background */\n#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n/* Label box, darken color on hover, fitted */\n#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Estimator label */\n\n#sk-container-id-1 div.sk-label label {\n  font-family: monospace;\n  font-weight: bold;\n  display: inline-block;\n  line-height: 1.2em;\n}\n\n#sk-container-id-1 div.sk-label-container {\n  text-align: center;\n}\n\n/* Estimator-specific */\n#sk-container-id-1 div.sk-estimator {\n  font-family: monospace;\n  border: 1px dotted var(--sklearn-color-border-box);\n  border-radius: 0.25em;\n  box-sizing: border-box;\n  margin-bottom: 0.5em;\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-1 div.sk-estimator.fitted {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n/* on hover */\n#sk-container-id-1 div.sk-estimator:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-1 div.sk-estimator.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Specification for estimator info (e.g. \"i\" and \"?\") */\n\n/* Common style for \"i\" and \"?\" */\n\n.sk-estimator-doc-link,\na:link.sk-estimator-doc-link,\na:visited.sk-estimator-doc-link {\n  float: right;\n  font-size: smaller;\n  line-height: 1em;\n  font-family: monospace;\n  background-color: var(--sklearn-color-background);\n  border-radius: 1em;\n  height: 1em;\n  width: 1em;\n  text-decoration: none !important;\n  margin-left: 0.5em;\n  text-align: center;\n  /* unfitted */\n  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n  color: var(--sklearn-color-unfitted-level-1);\n}\n\n.sk-estimator-doc-link.fitted,\na:link.sk-estimator-doc-link.fitted,\na:visited.sk-estimator-doc-link.fitted {\n  /* fitted */\n  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n  color: var(--sklearn-color-fitted-level-1);\n}\n\n/* On hover */\ndiv.sk-estimator:hover .sk-estimator-doc-link:hover,\n.sk-estimator-doc-link:hover,\ndiv.sk-label-container:hover .sk-estimator-doc-link:hover,\n.sk-estimator-doc-link:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\ndiv.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n.sk-estimator-doc-link.fitted:hover,\ndiv.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n.sk-estimator-doc-link.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\n/* Span, style for the box shown on hovering the info icon */\n.sk-estimator-doc-link span {\n  display: none;\n  z-index: 9999;\n  position: relative;\n  font-weight: normal;\n  right: .2ex;\n  padding: .5ex;\n  margin: .5ex;\n  width: min-content;\n  min-width: 20ex;\n  max-width: 50ex;\n  color: var(--sklearn-color-text);\n  box-shadow: 2pt 2pt 4pt #999;\n  /* unfitted */\n  background: var(--sklearn-color-unfitted-level-0);\n  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n}\n\n.sk-estimator-doc-link.fitted span {\n  /* fitted */\n  background: var(--sklearn-color-fitted-level-0);\n  border: var(--sklearn-color-fitted-level-3);\n}\n\n.sk-estimator-doc-link:hover span {\n  display: block;\n}\n\n/* \"?\"-specific style due to the `<a>` HTML tag */\n\n#sk-container-id-1 a.estimator_doc_link {\n  float: right;\n  font-size: 1rem;\n  line-height: 1em;\n  font-family: monospace;\n  background-color: var(--sklearn-color-background);\n  border-radius: 1rem;\n  height: 1rem;\n  width: 1rem;\n  text-decoration: none;\n  /* unfitted */\n  color: var(--sklearn-color-unfitted-level-1);\n  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n}\n\n#sk-container-id-1 a.estimator_doc_link.fitted {\n  /* fitted */\n  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n  color: var(--sklearn-color-fitted-level-1);\n}\n\n/* On hover */\n#sk-container-id-1 a.estimator_doc_link:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\n#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-3);\n}\n</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>OrdinalEncoder(handle_unknown=&#x27;use_encoded_value&#x27;, unknown_value=-1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>OrdinalEncoder</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.preprocessing.OrdinalEncoder.html\">?<span>Documentation for OrdinalEncoder</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>OrdinalEncoder(handle_unknown=&#x27;use_encoded_value&#x27;, unknown_value=-1)</pre></div> </div></div></div></div>"},"metadata":{}}],"execution_count":4},{"cell_type":"markdown","source":"train_eng에 train_df를 이용해 위의 함수를 이용해 가공한 데이터 넣음  \nall_num_cols, all_cat_cols로 더 만들어진 컬럼을 다시 분류  \n\nStandardScaler: 각 컬럼에서 평균과 표준편차를 학습  \nfit으로 계산해서 기억해 놓음  \n\nOrdinalEncoder: CAT를 숫자로 바꿔줌  \nhandle_unknown, unknown_values=-1 학습데이터에 없는 데이터가 test 데이터에서 갑자기 나오면 -1로 표시하고 넘어감  \nfit으로 각각 CAT에 맞는 번호를 만들어 놓음 (A학교는 0, B학교는 1)","metadata":{}},{"cell_type":"code","source":"def preprocess_pipeline_separate(df):\n    df_eng = add_engineered_features(df)\n    # Numerical: scale\n    nums_scaled = scaler.transform(df_eng[all_num_cols])\n    # Categorical: encode to integers\n    cats_encoded = encoder.transform(df_eng[all_cat_cols]).astype(np.int64)\n    return nums_scaled, cats_encoded","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T13:28:38.182088Z","iopub.execute_input":"2026-01-30T13:28:38.182369Z","iopub.status.idle":"2026-01-30T13:28:38.187645Z","shell.execute_reply.started":"2026-01-30T13:28:38.182345Z","shell.execute_reply":"2026-01-30T13:28:38.186489Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"앞서 만들어둔 Feature_Engineering, Scaler, Encoder을 사용해  \n실제 데이터를 가공 // nums 와 cats 컬럼을 따로 분류  \n\n나중에 test data에서 사용가능, fit은 train에서 한걸로 (data leakage)  ","metadata":{}},{"cell_type":"code","source":"X_num, X_cat = preprocess_pipeline_separate(train_df)\ny = train_df[TARGET].values\nX_test_num, X_test_cat = preprocess_pipeline_separate(test_df)\nX_orig_num, X_orig_cat = preprocess_pipeline_separate(original_df)\ny_original = original_df[TARGET].values\n\ncat_unique_counts = []\nfor i, col in enumerate(all_cat_cols):\n    n_unique = int(encoder.categories_[i].size)\n    cat_unique_counts.append(n_unique)\n\nprint(\"Categorical feature cardinalities:\", cat_unique_counts)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T13:28:38.188781Z","iopub.execute_input":"2026-01-30T13:28:38.189627Z","iopub.status.idle":"2026-01-30T13:28:41.999187Z","shell.execute_reply.started":"2026-01-30T13:28:38.189602Z","shell.execute_reply":"2026-01-30T13:28:41.998152Z"}},"outputs":[{"name":"stdout","text":"Categorical feature cardinalities: [8, 3, 7, 792, 617, 2, 66, 3, 5, 3, 3]\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"Scikit-Learn 도구들은 기본적으로 입력이 DataFrame이여도  \n출력은 Numpy array  \n위 데이터들은 numpy array들임  \n\ntrain_df: AI가 생성한 가짜 데이터\noriginal_df: 그 가짜를 만들 때 참고했던 진짜 원본 데이터\n\ncat_unique_counts 임베딩 크기 계산  \n각 범주형 변수마다 몇 종류의 데이터가 있는지 저장 (남여 2개 인지,  \na,b,c 3개인지 등등  ","metadata":{}},{"cell_type":"code","source":"class SEBlock(nn.Module):\n    def __init__(self, channels, reduction=4):\n        super().__init__()\n        self.fc1 = nn.Linear(channels, channels // reduction, bias=False)\n        self.relu = nn.ReLU()\n        self.fc2 = nn.Linear(channels // reduction, channels, bias=False)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        # x: (batch, channels)\n        se = x.mean(dim=0, keepdim=True)  # global avg pool -> (1, channels)\n        se = self.fc1(se)\n        se = self.relu(se)\n        se = self.fc2(se)\n        se = self.sigmoid(se)\n        return x * se  # broadcast","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T13:28:42.001701Z","iopub.execute_input":"2026-01-30T13:28:42.001988Z","iopub.status.idle":"2026-01-30T13:28:42.008322Z","shell.execute_reply.started":"2026-01-30T13:28:42.001965Z","shell.execute_reply":"2026-01-30T13:28:42.007361Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"SEBlock: 어떤 feature가 중요한지 스스로 공부해서, 중요한 건 키우고  \n중요하지 않는건 끄는 역할  \n\nnn.Module: PyTorch 모델이 갖춰야할 모든 기본 소양(파라미터 관리,  \nGPU이동, 저장/로드 등)을 미리 만들어 둔 기본 틀  \n이 틀을 가져다가 그 안에 원하는 함수 선언  \n\nchannels: 입력 데이터 갯수, reduction: 압축, 기본값 4  \nfc1 => 입력 channels/reduction 만큼 줄임 (알아서 학습해서 줄임)  \nrelu => 음수는 0  \nfc2 => 줄어든걸 다시 원상 복귀  \nsigmoid => 숫자를 0~1사이로 바꿈 (각 특징의 중요도를 점수로)  \n\nse = x.mean => 들어온 데이터를 세로 방향(dim=0)으로 평균 냄  \nex) (10,20) -> (1,20)  \n\nse값이 높으면 정보가 통과하고, 낮으면 차단됨  중요도가 높은 것만 남음  ","metadata":{}},{"cell_type":"code","source":"class ResidualBlock(nn.Module):\n    def __init__(self, dim, dropout=0.1, reduction=4):\n        super().__init__()\n        self.linear1 = nn.Linear(dim, dim)\n        self.linear2 = nn.Linear(dim, dim)\n        self.dropout = nn.Dropout(dropout)\n        self.norm1 = nn.LayerNorm(dim)\n        self.norm2 = nn.LayerNorm(dim)\n        self.se = SEBlock(dim, reduction=reduction)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        residual = x\n        # First sub-block\n        out = self.norm1(x)\n        out = self.linear1(out)\n        out = self.relu(out)\n        out = self.dropout(out)\n        # Second sub-block\n        out = self.norm2(out)\n        out = self.linear2(out)\n        out = self.dropout(out)\n        # SE\n        out = self.se(out)\n        # Residual connection\n        out = out + residual\n        return out","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T13:28:42.009508Z","iopub.execute_input":"2026-01-30T13:28:42.009822Z","iopub.status.idle":"2026-01-30T13:28:42.029491Z","shell.execute_reply.started":"2026-01-30T13:28:42.009798Z","shell.execute_reply":"2026-01-30T13:28:42.028505Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"residual block: 원본 + residual  \n\nLinear(dim, dim) 차원을 줄이지 않고 정보를 조합하여 더 좋은 정보를 만듬\n\nDropout -> 학습할때 뇌세포의 10%(0.1)을 랜덤으로 꺼버림  \n특정 뉴런 하나에 의존하지 말고 다 같이 이용하라는 뜻 (과적합 방지) \nex) ReLU에서 나온 데이터를 10%는 꺼지게 함  \n\nLayerNorm -> 데이터가 레이어를 지날 때마다 값이 너무 커지거나 작아지지  \n않게, 평균 0, 분산 1로 계속 맞춰줌, 학습의 안정성  ","metadata":{}},{"cell_type":"code","source":"class TabularResNetWithEmbedding(nn.Module):\n    def __init__(\n            self,\n            num_numerical,\n            cat_unique_counts,\n            embedding_dim=8,\n            hidden_dim=256,\n            n_blocks=4,\n            dropout=0.1,\n            head_dims=[64, 16]\n    ):\n        super().__init__() # 파이토치 기능 물리받기\n        self.num_numerical = num_numerical\n        self.embedding_dim = embedding_dim\n\n        # Embedding layers for each categorical feature\n        # ModuleList: 파이토치 전용 리스트\n        # nn.Embedding: 단어장, \n        # cat에서 몇개 종류가 있는지 계산했던 걸 바탕으로 embedding_dim에 만들어줌\n        # ex) A학교가 1이였는데, [-2.2, 0.1 ... ]등 8개 숫자를 갖는 차원으로 바꿔줌\n        # 처음에는 랜덤이지만 AI가 학습\n        # n_cat +1로 모르는 값(-1) 용 자리도 추가\n        self.embeddings = nn.ModuleList([\n            nn.Embedding(n_cat + 1, embedding_dim, padding_idx=-1)  # -1 mapped to last index\n            for n_cat in cat_unique_counts\n        ])\n\n        total_cat_dim = len(cat_unique_counts) * embedding_dim\n        # input_dim은 원래 있던 숫자 데이터 개수 + embedding을 거쳐 숫자로 변신한\n        # 범주형 데이터들의 총 길이\n        input_dim = num_numerical + total_cat_dim\n        \n\n        # Projection to hidden_dim\n        # 입력 데이터가 몇개든 상관없이 모델 내부에서 사용할 크기로 통일 (256)\n        self.proj = nn.Linear(input_dim, hidden_dim)\n        # 데이터를 모델에 넣기 전에 일부러 훼손, 특정 변수 의존도 낮추기, 실전 데이터 노이즈 방지\n        self.dropout_in = nn.Dropout(dropout)\n\n        # Residual blocks\n        # ResidualBlock을 n_blocks개(4개) 만큼 쌓아 올림\n        self.blocks = nn.Sequential(\n            *[ResidualBlock(hidden_dim, dropout=dropout) for _ in range(n_blocks)]\n        )\n\n        # Prediction head\n        # 최종 256가지를 정리해서 점수를 내뱉는 (1개 output)과정\n        layers = []\n        prev = hidden_dim\n        # hidden_dim = 256, head_dims = [64, 16] 이라면\n        # nn.Linear(256, 64) -> nn.Linear(64, 16)\n        for h in head_dims:\n            layers.extend([\n                nn.Linear(prev, h),\n                nn.ReLU(),\n                nn.Dropout(dropout)\n            ])\n            prev = h\n        # 마지막 결과물을 1차원으로 줄임\n        layers.append(nn.Linear(prev, 1))\n        # self.head는 forward함수 안에서 호출\n        self.head = nn.Sequential(*layers)\n\n    def forward(self, x_num, x_cat):\n        # 입력을 2개로 받아 따로 처리\n        # x_num: (B, num_numerical)\n        # x_cat: (B, n_cats)\n        batch_size = x_num.size(0)\n\n        # Embed categorical features\n        x_embeds = []\n        # 범주형 데이터 변환\n        for i, emb in enumerate(self.embeddings):\n            # x_cat[:, i] shape: (B,)\n            # i번째 범주형 변수만 꺼냄\n            xi = x_cat[:, i]\n            # Handle -1 (unknown): map to last embedding index\n            # 모르는 값 -1 처리\n            xi = torch.where(xi == -1, torch.tensor(emb.num_embeddings - 1, device=xi.device), xi)\n            embed_i = emb(xi)  # (B, embedding_dim)\n            # 단어장을 찾아서 숫자로 변환\n            x_embeds.append(embed_i)\n\n        # 변환된 범주형 데이터를 옆으로 이어 붙이고, 수치형 데이터도 붙임\n        x_cat_emb = torch.cat(x_embeds, dim=1)  # (B, total_cat_dim)\n        # Concat numerical and embedded categorical\n        x = torch.cat([x_num, x_cat_emb], dim=1)  # (B, input_dim)\n\n        # Project to hidden space\n        # 규격 맞추고 dropout 10%\n        x = self.proj(x)\n        x = self.dropout_in(x)\n\n        # Residual blocks\n        # residual block 4번 통과\n        x = self.blocks(x)\n\n        # Prediction head\n        # (n,1) 차원 n으로 펴서 return\n        out = self.head(x).squeeze(1)\n        return out","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T13:28:42.030569Z","iopub.execute_input":"2026-01-30T13:28:42.030799Z","iopub.status.idle":"2026-01-30T13:28:42.051712Z","shell.execute_reply.started":"2026-01-30T13:28:42.030774Z","shell.execute_reply":"2026-01-30T13:28:42.050750Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"def train_model(model, train_loader, val_loader, epochs=200, lr=1e-3, weight_decay=1e-5, patience=20, factor=0.5,\n                min_lr=1e-6):\n    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n        optimizer, mode='min', factor=factor, patience=patience // 2, min_lr=min_lr\n    )\n    # epochs = 200, lr = 1/1000, patience: 연속 20번 점수가 안오르면 종료\n    # factor = 0.5, 정체되면 학습률을 절반(0.5)으로 줄임\n    # optimizer: AdamW 모델의 파라미터를 어떻게 고칠지 결정\n    # scheduler: ReduceLROnPlateau: 결과가 평평해지면 학습률을 줄여라\n\n    # critetion: MSE\n    criterion = nn.MSELoss()\n\n    # best_val_loss: 지금까지 최고 기록, inf로 초기화\n    best_val_loss = float('inf')\n    patience_counter = 0\n    # best_val에서의 모델 상태(가중치) 저장\n    best_weights = None\n\n    for epoch in range(epochs):\n        # trian mode on, Dropout 켜짐, BatchNorm 켜짐(데이터 통계 업데이트)\n        model.train()\n        train_loss = 0.0\n        for xb_num, xb_cat, yb in train_loader:\n        # 숫자데이터, 범주데이터, 정답지\n            xb_num, xb_cat, yb = xb_num.to(device), xb_cat.to(device), yb.to(device)\n            # 이전 문제 풀 때 했던 내용 지움\n            optimizer.zero_grad()\n            # 예측\n            pred = model(xb_num, xb_cat)\n            # 채점\n            loss = criterion(pred, yb)\n            # 역추적\n            loss.backward()\n            # 수정\n            optimizer.step()\n            # 이번 epoch의 loss 기록\n            train_loss += loss.item()\n\n        # eval: 실제 시험, dropout 끔\n        model.eval()\n        val_loss = 0.0\n        # tocrch.no_grad: 정답만 나오면 되므로(학습 불필요) 계산 기록 남기지 않음\n        with torch.no_grad():\n            for xb_num, xb_cat, yb in val_loader:\n                xb_num, xb_cat, yb = xb_num.to(device), xb_cat.to(device), yb.to(device)\n                pred = model(xb_num, xb_cat)\n                loss = criterion(pred, yb)\n                val_loss += loss.item()\n        # RMSE\n        val_loss /= len(val_loader)\n        val_rmse = val_loss ** 0.5\n        # scheduler에 성적 보고\n        scheduler.step(val_loss)\n\n        # 가장 잘 맞췄떤 순간을 기록\n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n            best_val_rmse = val_rmse  # best RMSE\n            # 카운트 초기화\n            patience_counter = 0\n            # state_dict 상태 기록\n            best_weights = model.state_dict()\n        else:\n            patience_counter += 1\n            if patience_counter >= patience:\n                break\n\n        # 5번마다 RMSE 출력\n        if (epoch + 1) % 5 == 0:\n            print(f\"Epoch {epoch + 1}/{epochs} | Val RMSE: {val_rmse:.5f}\")\n\n    # best_weight 당시의 파라미터를 모델에 옮겨줌\n    if best_weights is not None:\n        model.load_state_dict(best_weights)\n    return model, best_val_rmse","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T13:28:42.053147Z","iopub.execute_input":"2026-01-30T13:28:42.053479Z","iopub.status.idle":"2026-01-30T13:28:42.071484Z","shell.execute_reply.started":"2026-01-30T13:28:42.053453Z","shell.execute_reply":"2026-01-30T13:28:42.070613Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"n_splits = 5\nkf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n\ntest_predictions = []\noof_predictions = np.zeros(len(y))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T13:28:42.072251Z","iopub.execute_input":"2026-01-30T13:28:42.072419Z","iopub.status.idle":"2026-01-30T13:28:42.091439Z","shell.execute_reply.started":"2026-01-30T13:28:42.072402Z","shell.execute_reply":"2026-01-30T13:28:42.090768Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"5fold validation  \n\ntest_prediction: Test Set에 대한 예측값을 저장할 리스트 (1,2,3.. 모델  \noof_predictions: Out-Of-Fold: 학습하지 않았던 데이터를 예측한거  \n(1,2,3,4로 학습했을때의 5번 저장)  ","metadata":{}},{"cell_type":"code","source":"print(f\"Starting {n_splits}-fold CV with advanced TabularResNet...\")\n\n# kf를 이용해 데이터를 5가지 방법으로 잘라줌\nfor fold, (train_idx, val_idx) in enumerate(kf.split(X_num, y)):\n    print(f\"\\n--- Fold {fold + 1}/{n_splits} ---\")\n\n    # Split\n    # index를 보고 실제 데이터를 가져옴\n    X_num_train, X_cat_train = X_num[train_idx], X_cat[train_idx]\n    y_train = y[train_idx]\n    X_num_val, X_cat_val = X_num[val_idx], X_cat[val_idx]\n    y_val = y[val_idx]\n\n    # Augment with original data\n    # original 데이터도 합쳐서 학습 데이터를 늘려줌\n    X_num_combined = np.vstack([X_num_train, X_orig_num])\n    X_cat_combined = np.vstack([X_cat_train, X_orig_cat])\n    y_combined = np.concatenate([y_train, y_original])\n\n    # Tensors\n    # 파이토치용 포장 (tensor 변환)\n    X_num_train_t = torch.tensor(X_num_combined, dtype=torch.float32)\n    # cat 컬럼은 nn.Embedding에서 정수만 입력으로 받음, 실수 불가\n    X_cat_train_t = torch.tensor(X_cat_combined, dtype=torch.int64)\n    y_train_t = torch.tensor(y_combined, dtype=torch.float32)\n\n    X_num_val_t = torch.tensor(X_num_val, dtype=torch.float32)\n    X_cat_val_t = torch.tensor(X_cat_val, dtype=torch.int64)\n    y_val_t = torch.tensor(y_val, dtype=torch.float32)\n\n    X_test_num_t = torch.tensor(X_test_num, dtype=torch.float32)\n    X_test_cat_t = torch.tensor(X_test_cat, dtype=torch.int64)\n\n    # Datasets & Loaders\n    # TensorDataset은 숫자, 범주, 정답을 하나로 만들어줌\n    train_ds = TensorDataset(X_num_train_t, X_cat_train_t, y_train_t)\n    val_ds = TensorDataset(X_num_val_t, X_cat_val_t, y_val_t)\n    # DataLoader을 이용해 전체 데이터를 batch_Size만큼 상자에 담아줌\n    train_loader = DataLoader(train_ds, batch_size=256, shuffle=True)\n    val_loader = DataLoader(val_ds, batch_size=1024, shuffle=False)\n\n    # Model\n    # 모델 초기화\n    # 학습하기 전에 모델을 초기화해서 이전 데이터를 리셋\n    model = TabularResNetWithEmbedding(\n        num_numerical=X_num.shape[1],\n        cat_unique_counts=cat_unique_counts,\n        embedding_dim=8,\n        hidden_dim=256,\n        n_blocks=3,\n        dropout=0.11,\n        head_dims=[64, 16]\n    ).to(device)\n\n    # Train\n    model, best_rmse = train_model(\n        model,\n        train_loader,\n        val_loader,\n        epochs=300,\n        lr=1e-3,\n        weight_decay=1e-4,\n        patience=20,\n        factor=0.5,\n        min_lr=1e-6\n    )\n\n    # Predict\n    model.eval()\n    with torch.no_grad():\n        val_pred = model(X_num_val_t.to(device), X_cat_val_t.to(device)).cpu().numpy()\n        # test_pred 이번 모델 예측치를 구해줌\n        test_pred = model(X_test_num_t.to(device), X_test_cat_t.to(device)).cpu().numpy()\n\n    # validation set의 예측값을 oof에 따로 담아줌\n    oof_predictions[val_idx] = val_pred\n    # 이번 모델이 생각한 test 정답 데이터를 리스트에 추가\n    test_predictions.append(test_pred)\n\n    print(f\"Fold {fold + 1} RMSE: {best_rmse:.5f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T13:28:42.092524Z","iopub.execute_input":"2026-01-30T13:28:42.092742Z","iopub.status.idle":"2026-01-30T13:41:50.394831Z","shell.execute_reply.started":"2026-01-30T13:28:42.092715Z","shell.execute_reply":"2026-01-30T13:41:50.393831Z"}},"outputs":[{"name":"stdout","text":"Starting 5-fold CV with advanced TabularResNet...\n\n--- Fold 1/5 ---\nEpoch 5/300 | Val RMSE: 8.87059\nEpoch 10/300 | Val RMSE: 8.73573\nEpoch 15/300 | Val RMSE: 8.75876\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_55/1828896547.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;31m# Train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m     model, best_rmse = train_model(\n\u001b[0m\u001b[1;32m     57\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_55/2124673890.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, epochs, lr, weight_decay, patience, factor, min_lr)\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0;31m# 예측\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxb_cat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m             \u001b[0;31m# 채점\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_55/2768622289.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x_num, x_cat)\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;31m# Residual blocks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;31m# residual block 4번 통과\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0;31m# Prediction head\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_55/2754802220.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mresidual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m# First sub-block\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/normalization.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m         return F.layer_norm(\n\u001b[0m\u001b[1;32m    218\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalized_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlayer_norm\u001b[0;34m(input, normalized_shape, weight, bias, eps)\u001b[0m\n\u001b[1;32m   2903\u001b[0m             \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2904\u001b[0m         )\n\u001b[0;32m-> 2905\u001b[0;31m     return torch.layer_norm(\n\u001b[0m\u001b[1;32m   2906\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalized_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2907\u001b[0m     )\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":12},{"cell_type":"code","source":"# ==============================\n# Final Evaluation & Submission\n# ==============================\n# 5fold 에서 나온 답을 실제 정답과 비교\noof_rmse = np.sqrt(mean_squared_error(y, oof_predictions))\nprint(\"\\n\" + \"=\" * 50)\nprint(f\"Final OOF RMSE: {oof_rmse:.6f}\")\nprint(\"=\" * 50)\n\n# TARGET: score\n# oof_df를 나중에 다른 모델과 합칠때 사용할 수도 있으니 저장\noof_df = pd.DataFrame({'id': train_df['id'], TARGET: oof_predictions})\noof_df.to_csv('nn_oof.csv', index=False)\n\n# 5개 모델의 평균을 내서 마지막 점수를 구함\nsubmission_df[TARGET] = np.mean(test_predictions, axis=0)\nsubmission_df.to_csv('submission.csv', index=False)\nprint(\"\\nSubmission saved!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T13:41:50.396035Z","iopub.status.idle":"2026-01-30T13:41:50.396378Z","shell.execute_reply.started":"2026-01-30T13:41:50.396230Z","shell.execute_reply":"2026-01-30T13:41:50.396246Z"}},"outputs":[],"execution_count":null}]}