{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":119082,"databundleVersionId":14993753,"sourceType":"competition"},{"sourceId":13904981,"sourceType":"datasetVersion","datasetId":8762382}],"dockerImageVersionId":31234,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"V1-V22 uses the lightgbm model, with the best version being V11.\n\nFrom V23ï¼Œ try other models","metadata":{}},{"cell_type":"markdown","source":"reference: https://www.kaggle.com/jiaoyouzhang","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import OrdinalEncoder, StandardScaler\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import TensorDataset, DataLoader","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"OrdialEncoder: ë¬¸ì(ì¹´í…Œê³ ë¦¬)ë°ì´í„°ë¥¼ ìˆ«ìë¡œ ('S','M' -> 0,1)  \nStandardScalar: ë°ì´í„° ë‹¨ìœ„ í†µì¼ (í‰ê·  0, ë¶„ì‚° 1)\n\ntorch: PyTorch  \ntorch.nn as nn: ì‹ ê²½ë§ì˜ layerì™€ êµ¬ì¡°ë¥¼ ë§Œë“¬  \noptim: ìµœì í™” ë„êµ¬ (Adam, SGD)  \nTensorDataset: ì…ë ¥ ë°ì´í„°ì™€ ì •ë‹µì„ í•˜ë‚˜ë¡œ ë¬¶ì–´ì¤Œ  \nDataLoader: ë¬¶ì¸ ë°ì´í„°ë¥¼ í•™ìŠµí•˜ê¸° ì¢‹ê²Œ batch  ","metadata":{}},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\ntrain_file = \"/kaggle/input/playground-series-s6e1/train.csv\"\ntest_file = \"/kaggle/input/playground-series-s6e1/test.csv\"\noriginal_file = \"/kaggle/input/exam-score-prediction-dataset/Exam_Score_Prediction.csv\"\n\ntrain_df = pd.read_csv(train_file)\ntest_df = pd.read_csv(test_file)\noriginal_df = pd.read_csv(original_file) \nsubmission_df = pd.read_csv(\"/kaggle/input/playground-series-s6e1/sample_submission.csv\") \nTARGET = 'exam_score'\n\nnum_features = ['study_hours', 'class_attendance', 'sleep_hours']\nbase_features = [col for col in train_df.columns if col not in [TARGET, 'id']]\nCATS = base_features\nNUMS = num_features  # only these are truly numerical","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"í˜„ì¬ ì»´í“¨í„°ì— CUDAê°€ ì‡ëŠ”ì§€ í™•ì¸í•˜ê³  ìˆìœ¼ë©´ CUDA, ì—†ìœ¼ë©´ CPU ì„ íƒ  \n\ntrian, test, original fileì„ ê°€ì ¸ì˜´\n\nTARGETë³€ìˆ˜ ëª©í‘œë¥¼ ë„£ê³ ,  \nnum_featuresì— ìˆ«ì ë°ì´í„°  \nbase_featuresì— ëª¨ë“  ì»¬ëŸ¼ (target, id ì œì™¸)  \n\nCATS: ë²”ì£¼í˜•  \nNUMS: ìˆ«ìí˜•  ","metadata":{}},{"cell_type":"code","source":"def add_engineered_features(df):\n    df_temp = df.copy()\n    # Sine features\n    df_temp['_study_hours_sin'] = np.sin(2 * np.pi * df_temp['study_hours'] / 12).astype('float32')\n    df_temp['_class_attendance_sin'] = np.sin(2 * np.pi * df_temp['class_attendance'] / 12).astype('float32')\n\n    for col in num_features:\n        if col in df_temp.columns:\n            df_temp[f'log_{col}'] = np.log1p(df_temp[col])\n            df_temp[f'{col}_sq'] = df_temp[col] ** 2\n\n    for col in train_df.select_dtypes(include=['object','category']).columns.tolist():\n        cat_series = df_temp[col].astype(str)\n        freq_map = cat_series.value_counts().to_dict()\n        df_temp[f\"{col}_freq\"] = cat_series.map(freq_map).fillna(0).astype(int)\n        \n    # Linear combo feature\n    df_temp['feature_formula'] = (\n            5.9051154511950499 * df_temp['study_hours'] +\n            0.34540967058057986 * df_temp['class_attendance'] +\n            1.423461171860262 * df_temp['sleep_hours'] + 4.7819\n    )\n\n    # Keep categorical as string for encoding\n    for col in CATS:\n        df_temp[col] = df_temp[col].astype(str)\n\n    return df_temp","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"ìƒˆë¡œìš´ ë°ì´í„° ë§Œë“œëŠ” Feature Engineering  \n\n_study_hour_sinì´ë¼ëŠ” ìƒˆë¡œìš´ ì»¬ëŸ¼ì„ ë§Œë“¤ìŒ \nê³µë¶€ë¥¼ í•˜ëŠ” ì‹œê°„ì— ë‹¤ë¥¸ íŒ¨í„´ì´ ìˆë‚˜? (0-12ì‹œê°„ì„ í•œ ì£¼ê¸°ë¡œ ë³´ê³ )  \nì‹¤í—˜ì ì¸ ì˜ë„ë¼ê³  ìƒê°í•¨  \n\nfor col in num_features: ìˆ˜ì¹˜í˜• ë°ì´í„° ë³€í™˜(ë¡œê·¸ & ì œê³±)  \nìˆ˜ì¹˜í˜• ë°ì´í„°ëŠ” ê°ê° logì™€ ì œê³± ì»¬ëŸ¼ì„ ì¶”ê°€  \n\nfor col in train_df.select_dtypes  \nì›ë³¸ í•™ìŠµ ë°ì´í„°(trian_df)ì—ì„œ ê¸€ì(object)ë‚˜ ë²”ì£¼(cat)ì¸ ì»¬ëŸ¼  \ncat_series => í•´ë‹¹ ì—´ì„ í™•ì‹¤í•˜ê²Œ ë¬¸ìì—´ë¡œ  \nfreq_map => í•´ë‹¹ ì—´ì—ì„œ ê° í•­ëª©ì´ ëª‡ë²ˆ ë‚˜ì™”ëŠ”ì§€ dictë¡œ ë³€í™˜  \n{col}_freq ì»¬ëŸ¼ì— ê° ì»¬ëŸ¼ì˜ ë¹ˆë„ìˆ˜ë¥¼ ìˆ«ìë¡œ ë°”ê¿”ì„œ ì €ì¥, ë¹ˆì¹¸ì€ 0ìœ¼ë¡œ ì±„ìš°ê³  nullë¡œ ë³€í™˜  \nex) Aí•™êµ -> 50, Bí•™êµ -> 10, Aí•™êµ -> 50  \n\nfeature_formula: ì‘ì„±ìê°€ ì´ ëª¨ë¸ì„ ë§Œë“¤ê¸° ì „ì— linear regressionê°™ì€  \në‹¨ìˆœí•œ ëª¨ë¸ì„ ë¨¼ì € ëŒë ¤ì—‡ ì–»ì€ ìµœì ì˜ ê³„ìˆ˜  \n\nfor col in CATS:  \në²”ì£¼í˜• ë°ì´í„°ë¥¼ ë¬¸ìì—´ë¡œ ê³ ì •í•˜ê³  return  ","metadata":{}},{"cell_type":"code","source":"train_eng = add_engineered_features(train_df)\n\nall_num_cols = [col for col in train_eng.columns if col not in CATS + [TARGET, 'id']]\nall_cat_cols = CATS\n\nscaler = StandardScaler()\nscaler.fit(train_eng[all_num_cols])\n\nencoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\nencoder.fit(train_eng[all_cat_cols])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"train_engì— train_dfë¥¼ ì´ìš©í•´ ìœ„ì˜ í•¨ìˆ˜ë¥¼ ì´ìš©í•´ ê°€ê³µí•œ ë°ì´í„° ë„£ìŒ  \nall_num_cols, all_cat_colsë¡œ ë” ë§Œë“¤ì–´ì§„ ì»¬ëŸ¼ì„ ë‹¤ì‹œ ë¶„ë¥˜  \n\nStandardScaler: ê° ì»¬ëŸ¼ì—ì„œ í‰ê· ê³¼ í‘œì¤€í¸ì°¨ë¥¼ í•™ìŠµ  \nfitìœ¼ë¡œ ê³„ì‚°í•´ì„œ ê¸°ì–µí•´ ë†“ìŒ  \n\nOrdinalEncoder: CATë¥¼ ìˆ«ìë¡œ ë°”ê¿”ì¤Œ  \nhandle_unknown, unknown_values=-1 í•™ìŠµë°ì´í„°ì— ì—†ëŠ” ë°ì´í„°ê°€ test ë°ì´í„°ì—ì„œ ê°‘ìê¸° ë‚˜ì˜¤ë©´ -1ë¡œ í‘œì‹œí•˜ê³  ë„˜ì–´ê°  \nfitìœ¼ë¡œ ê°ê° CATì— ë§ëŠ” ë²ˆí˜¸ë¥¼ ë§Œë“¤ì–´ ë†“ìŒ (Aí•™êµëŠ” 0, Bí•™êµëŠ” 1)","metadata":{}},{"cell_type":"code","source":"def preprocess_pipeline_separate(df):\n    df_eng = add_engineered_features(df)\n    # Numerical: scale\n    nums_scaled = scaler.transform(df_eng[all_num_cols])\n    # Categorical: encode to integers\n    cats_encoded = encoder.transform(df_eng[all_cat_cols]).astype(np.int64)\n    return nums_scaled, cats_encoded","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"ì•ì„œ ë§Œë“¤ì–´ë‘” Feature_Engineering, Scaler, Encoderì„ ì‚¬ìš©í•´  \nì‹¤ì œ ë°ì´í„°ë¥¼ ê°€ê³µ // nums ì™€ cats ì»¬ëŸ¼ì„ ë”°ë¡œ ë¶„ë¥˜  \n\në‚˜ì¤‘ì— test dataì—ì„œ ì‚¬ìš©ê°€ëŠ¥, fitì€ trainì—ì„œ í•œê±¸ë¡œ (data leakage)  ","metadata":{}},{"cell_type":"code","source":"X_num, X_cat = preprocess_pipeline_separate(train_df)\ny = train_df[TARGET].values\nX_test_num, X_test_cat = preprocess_pipeline_separate(test_df)\nX_orig_num, X_orig_cat = preprocess_pipeline_separate(original_df)\ny_original = original_df[TARGET].values\n\ncat_unique_counts = []\nfor i, col in enumerate(all_cat_cols):\n    n_unique = int(encoder.categories_[i].size)\n    cat_unique_counts.append(n_unique)\n\nprint(\"Categorical feature cardinalities:\", cat_unique_counts)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Scikit-Learn ë„êµ¬ë“¤ì€ ê¸°ë³¸ì ìœ¼ë¡œ ì…ë ¥ì´ DataFrameì´ì—¬ë„  \nì¶œë ¥ì€ Numpy array  \nìœ„ ë°ì´í„°ë“¤ì€ numpy arrayë“¤ì„  \n\ntrain_df: AIê°€ ìƒì„±í•œ ê°€ì§œ ë°ì´í„°\noriginal_df: ê·¸ ê°€ì§œë¥¼ ë§Œë“¤ ë•Œ ì°¸ê³ í–ˆë˜ ì§„ì§œ ì›ë³¸ ë°ì´í„°\n\ncat_unique_counts ì„ë² ë”© í¬ê¸° ê³„ì‚°  \nê° ë²”ì£¼í˜• ë³€ìˆ˜ë§ˆë‹¤ ëª‡ ì¢…ë¥˜ì˜ ë°ì´í„°ê°€ ìˆëŠ”ì§€ ì €ì¥ (ë‚¨ì—¬ 2ê°œ ì¸ì§€,  \na,b,c 3ê°œì¸ì§€ ë“±ë“±  ","metadata":{}},{"cell_type":"code","source":"class SEBlock(nn.Module):\n    def __init__(self, channels, reduction=4):\n        super().__init__()\n        self.fc1 = nn.Linear(channels, channels // reduction, bias=False)\n        self.relu = nn.ReLU()\n        self.fc2 = nn.Linear(channels // reduction, channels, bias=False)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        # x: (batch, channels)\n        se = x.mean(dim=0, keepdim=True)  # global avg pool -> (1, channels)\n        se = self.fc1(se)\n        se = self.relu(se)\n        se = self.fc2(se)\n        se = self.sigmoid(se)\n        return x * se  # broadcast","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"SEBlock: ì–´ë–¤ featureê°€ ì¤‘ìš”í•œì§€ ìŠ¤ìŠ¤ë¡œ ê³µë¶€í•´ì„œ, ì¤‘ìš”í•œ ê±´ í‚¤ìš°ê³   \nì¤‘ìš”í•˜ì§€ ì•ŠëŠ”ê±´ ë„ëŠ” ì—­í•   \n\nnn.Module: PyTorch ëª¨ë¸ì´ ê°–ì¶°ì•¼í•  ëª¨ë“  ê¸°ë³¸ ì†Œì–‘(íŒŒë¼ë¯¸í„° ê´€ë¦¬,  \nGPUì´ë™, ì €ì¥/ë¡œë“œ ë“±)ì„ ë¯¸ë¦¬ ë§Œë“¤ì–´ ë‘” ê¸°ë³¸ í‹€  \nì´ í‹€ì„ ê°€ì ¸ë‹¤ê°€ ê·¸ ì•ˆì— ì›í•˜ëŠ” í•¨ìˆ˜ ì„ ì–¸  \n\nchannels: ì…ë ¥ ë°ì´í„° ê°¯ìˆ˜, reduction: ì••ì¶•, ê¸°ë³¸ê°’ 4  \nfc1 => ì…ë ¥ channels/reduction ë§Œí¼ ì¤„ì„ (ì•Œì•„ì„œ í•™ìŠµí•´ì„œ ì¤„ì„)  \nrelu => ìŒìˆ˜ëŠ” 0  \nfc2 => ì¤„ì–´ë“ ê±¸ ë‹¤ì‹œ ì›ìƒ ë³µê·€  \nsigmoid => ìˆ«ìë¥¼ 0~1ì‚¬ì´ë¡œ ë°”ê¿ˆ (ê° íŠ¹ì§•ì˜ ì¤‘ìš”ë„ë¥¼ ì ìˆ˜ë¡œ)  \n\nse = x.mean => ë“¤ì–´ì˜¨ ë°ì´í„°ë¥¼ ì„¸ë¡œ ë°©í–¥(dim=0)ìœ¼ë¡œ í‰ê·  ëƒ„  \nex) (10,20) -> (1,20)  \n\nseê°’ì´ ë†’ìœ¼ë©´ ì •ë³´ê°€ í†µê³¼í•˜ê³ , ë‚®ìœ¼ë©´ ì°¨ë‹¨ë¨  ì¤‘ìš”ë„ê°€ ë†’ì€ ê²ƒë§Œ ë‚¨ìŒ  ","metadata":{}},{"cell_type":"code","source":"class ResidualBlock(nn.Module):\n    def __init__(self, dim, dropout=0.1, reduction=4):\n        super().__init__()\n        self.linear1 = nn.Linear(dim, dim)\n        self.linear2 = nn.Linear(dim, dim)\n        self.dropout = nn.Dropout(dropout)\n        self.norm1 = nn.LayerNorm(dim)\n        self.norm2 = nn.LayerNorm(dim)\n        self.se = SEBlock(dim, reduction=reduction)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        residual = x\n        # First sub-block\n        out = self.norm1(x)\n        out = self.linear1(out)\n        out = self.relu(out)\n        out = self.dropout(out)\n        # Second sub-block\n        out = self.norm2(out)\n        out = self.linear2(out)\n        out = self.dropout(out)\n        # SE\n        out = self.se(out)\n        # Residual connection\n        out = out + residual\n        return out","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"residual block: ì›ë³¸ + residual  \n\nLinear(dim, dim) ì°¨ì›ì„ ì¤„ì´ì§€ ì•Šê³  ì •ë³´ë¥¼ ì¡°í•©í•˜ì—¬ ë” ì¢‹ì€ ì •ë³´ë¥¼ ë§Œë“¬\n\nDropout -> í•™ìŠµí• ë•Œ ë‡Œì„¸í¬ì˜ 10%(0.1)ì„ ëœë¤ìœ¼ë¡œ êº¼ë²„ë¦¼  \níŠ¹ì • ë‰´ëŸ° í•˜ë‚˜ì— ì˜ì¡´í•˜ì§€ ë§ê³  ë‹¤ ê°™ì´ ì´ìš©í•˜ë¼ëŠ” ëœ» (ê³¼ì í•© ë°©ì§€) \nex) ReLUì—ì„œ ë‚˜ì˜¨ ë°ì´í„°ë¥¼ 10%ëŠ” êº¼ì§€ê²Œ í•¨  \n\nLayerNorm -> ë°ì´í„°ê°€ ë ˆì´ì–´ë¥¼ ì§€ë‚  ë•Œë§ˆë‹¤ ê°’ì´ ë„ˆë¬´ ì»¤ì§€ê±°ë‚˜ ì‘ì•„ì§€ì§€  \nì•Šê²Œ, í‰ê·  0, ë¶„ì‚° 1ë¡œ ê³„ì† ë§ì¶°ì¤Œ, í•™ìŠµì˜ ì•ˆì •ì„±  ","metadata":{}},{"cell_type":"code","source":"class TabularResNetWithEmbedding(nn.Module):\n    def __init__(\n            self,\n            num_numerical,\n            cat_unique_counts,\n            embedding_dim=8,\n            hidden_dim=256,\n            n_blocks=4,\n            dropout=0.1,\n            head_dims=[64, 16]\n    ):\n        super().__init__()\n        self.num_numerical = num_numerical\n        self.embedding_dim = embedding_dim\n\n        # Embedding layers for each categorical feature\n        # ModuleList: íŒŒì´í† ì¹˜ ì „ìš© ë¦¬ìŠ¤íŠ¸\n        # nn.Embedding: ë‹¨ì–´ì¥, \n        # catì—ì„œ ëª‡ê°œ ì¢…ë¥˜ê°€ ìˆëŠ”ì§€ ê³„ì‚°í–ˆë˜ ê±¸ ë°”íƒ•ìœ¼ë¡œ embedding_dimì— ë§Œë“¤ì–´ì¤Œ\n        # ex) Aí•™êµê°€ 1ì´ì˜€ëŠ”ë°, [-2.2, 0.1 ... ]ë“± 8ê°œ ìˆ«ìë¥¼ ê°–ëŠ” ì°¨ì›ìœ¼ë¡œ ë°”ê¿”ì¤Œ\n        # ì²˜ìŒì—ëŠ” ëœë¤ì´ì§€ë§Œ AIê°€ í•™ìŠµ\n        self.embeddings = nn.ModuleList([\n            nn.Embedding(n_cat + 1, embedding_dim, padding_idx=-1)  # -1 mapped to last index\n            for n_cat in cat_unique_counts\n        ])\n\n        total_cat_dim = len(cat_unique_counts) * embedding_dim\n        # input_dimì€ ì›ë˜ ìˆë˜ ìˆ«ì ë°ì´í„° ê°œìˆ˜ + embeddingì„ ê±°ì³ ìˆ«ìë¡œ ë³€ì‹ í•œ\n        # ë²”ì£¼í˜• ë°ì´í„°ë“¤ì˜ ì´ ê¸¸ì´\n        input_dim = num_numerical + total_cat_dim\n        \n\n        # Projection to hidden_dim\n        # ì…ë ¥ ë°ì´í„°ê°€ ëª‡ê°œë“  ìƒê´€ì—†ì´ ëª¨ë¸ ë‚´ë¶€ì—ì„œ ì‚¬ìš©í•  í¬ê¸°ë¡œ í†µì¼ (256)\n        self.proj = nn.Linear(input_dim, hidden_dim)\n        # ë°ì´í„°ë¥¼ ëª¨ë¸ì— ë„£ê¸° ì „ì— ì¼ë¶€ëŸ¬ í›¼ì†, íŠ¹ì • ë³€ìˆ˜ ì˜ì¡´ë„ ë‚®ì¶”ê¸°, ì‹¤ì „ ë°ì´í„° ë…¸ì´ì¦ˆ ë°©ì§€\n        self.dropout_in = nn.Dropout(dropout)\n\n        # Residual blocks\n        # ResidualBlockì„ n_blocksê°œ(4ê°œ) ë§Œí¼ ìŒ“ì•„ ì˜¬ë¦¼\n        self.blocks = nn.Sequential(\n            *[ResidualBlock(hidden_dim, dropout=dropout) for _ in range(n_blocks)]\n        )\n\n        # Prediction head\n        # ìµœì¢… 256ê°€ì§€ë¥¼ ì •ë¦¬í•´ì„œ ì ìˆ˜ë¥¼ ë‚´ë±‰ëŠ” (1ê°œ output)ê³¼ì •\n        layers = []\n        prev = hidden_dim\n        for h in head_dims:\n            layers.extend([\n                nn.Linear(prev, h),\n                nn.ReLU(),\n                nn.Dropout(dropout)\n            ])\n            prev = h\n        layers.append(nn.Linear(prev, 1))\n        self.head = nn.Sequential(*layers)\n\n    def forward(self, x_num, x_cat):\n        # ì…ë ¥ì„ 2ê°œë¡œ ë°›ì•„ ë”°ë¡œ ì²˜ë¦¬\n        # x_num: (B, num_numerical)\n        # x_cat: (B, n_cats)\n        batch_size = x_num.size(0)\n\n        # Embed categorical features\n        x_embeds = []\n        # ë²”ì£¼í˜• ë°ì´í„° ë³€í™˜\n        for i, emb in enumerate(self.embeddings):\n            # x_cat[:, i] shape: (B,)\n            # ië²ˆì§¸ ë²”ì£¼í˜• ë³€ìˆ˜ë§Œ êº¼ëƒ„\n            xi = x_cat[:, i]\n            # Handle -1 (unknown): map to last embedding index\n            # ëª¨ë¥´ëŠ” ê°’ -1 ì²˜ë¦¬\n            xi = torch.where(xi == -1, torch.tensor(emb.num_embeddings - 1, device=xi.device), xi)\n            embed_i = emb(xi)  # (B, embedding_dim)\n            # ë‹¨ì–´ì¥ì„ ì°¾ì•„ì„œ ìˆ«ìë¡œ ë³€í™˜\n            x_embeds.append(embed_i)\n\n        # ë³€í™˜ëœ ë²”ì£¼í˜• ë°ì´í„°ë¥¼ ì˜†ìœ¼ë¡œ ì´ì–´ ë¶™ì´ê³ , ìˆ˜ì¹˜í˜• ë°ì´í„°ë„ ë¶™ì„\n        x_cat_emb = torch.cat(x_embeds, dim=1)  # (B, total_cat_dim)\n        # Concat numerical and embedded categorical\n        x = torch.cat([x_num, x_cat_emb], dim=1)  # (B, input_dim)\n\n        # Project to hidden space\n        # ê·œê²© ë§ì¶”ê³  dropout 10%\n        x = self.proj(x)\n        x = self.dropout_in(x)\n\n        # Residual blocks\n        # residual block 4ë²ˆ í†µê³¼\n        x = self.blocks(x)\n\n        # Prediction head\n        # (n,1) ì°¨ì› nìœ¼ë¡œ í´ì„œ return\n        out = self.head(x).squeeze(1)\n        return out","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\ndef train_model(model, train_loader, val_loader, epochs=200, lr=1e-3, weight_decay=1e-5, patience=20, factor=0.5,\n                min_lr=1e-6):\n    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n        optimizer, mode='min', factor=factor, patience=patience // 2, min_lr=min_lr\n    )\n    criterion = nn.MSELoss()\n\n    best_val_loss = float('inf')\n    patience_counter = 0\n    best_weights = None\n\n    for epoch in range(epochs):\n        model.train()\n        train_loss = 0.0\n        for xb_num, xb_cat, yb in train_loader:\n            xb_num, xb_cat, yb = xb_num.to(device), xb_cat.to(device), yb.to(device)\n            optimizer.zero_grad()\n            pred = model(xb_num, xb_cat)\n            loss = criterion(pred, yb)\n            loss.backward()\n            optimizer.step()\n            train_loss += loss.item()\n\n        model.eval()\n        val_loss = 0.0\n        with torch.no_grad():\n            for xb_num, xb_cat, yb in val_loader:\n                xb_num, xb_cat, yb = xb_num.to(device), xb_cat.to(device), yb.to(device)\n                pred = model(xb_num, xb_cat)\n                loss = criterion(pred, yb)\n                val_loss += loss.item()\n        val_loss /= len(val_loader)\n        val_rmse = val_loss ** 0.5\n        scheduler.step(val_loss)\n\n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n            best_val_rmse = val_rmse  # â† è®°å½• best RMSE\n            patience_counter = 0\n            best_weights = model.state_dict()\n        else:\n            patience_counter += 1\n            if patience_counter >= patience:\n                break\n\n        # ğŸ‘‡ æ¯ 10 ä¸ª epoch æ‰“å°ä¸€æ¬¡éªŒè¯ RMSE\n        if (epoch + 1) % 5 == 0:\n            print(f\"Epoch {epoch + 1}/{epochs} | Val RMSE: {val_rmse:.5f}\")\n\n    if best_weights is not None:\n        model.load_state_dict(best_weights)\n    return model, best_val_rmse\n\n\n# ==============================\n# K æŠ˜è®­ç»ƒ\n# ==============================\nn_splits = 5\nkf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n\ntest_predictions = []\noof_predictions = np.zeros(len(y))\n\nprint(f\"Starting {n_splits}-fold CV with advanced TabularResNet...\")\n\nfor fold, (train_idx, val_idx) in enumerate(kf.split(X_num, y)):\n    print(f\"\\n--- Fold {fold + 1}/{n_splits} ---\")\n\n    # Split\n    X_num_train, X_cat_train = X_num[train_idx], X_cat[train_idx]\n    y_train = y[train_idx]\n    X_num_val, X_cat_val = X_num[val_idx], X_cat[val_idx]\n    y_val = y[val_idx]\n\n    # Augment with original data\n    X_num_combined = np.vstack([X_num_train, X_orig_num])\n    X_cat_combined = np.vstack([X_cat_train, X_orig_cat])\n    y_combined = np.concatenate([y_train, y_original])\n\n    # Tensors\n    X_num_train_t = torch.tensor(X_num_combined, dtype=torch.float32)\n    X_cat_train_t = torch.tensor(X_cat_combined, dtype=torch.int64)\n    y_train_t = torch.tensor(y_combined, dtype=torch.float32)\n\n    X_num_val_t = torch.tensor(X_num_val, dtype=torch.float32)\n    X_cat_val_t = torch.tensor(X_cat_val, dtype=torch.int64)\n    y_val_t = torch.tensor(y_val, dtype=torch.float32)\n\n    X_test_num_t = torch.tensor(X_test_num, dtype=torch.float32)\n    X_test_cat_t = torch.tensor(X_test_cat, dtype=torch.int64)\n\n    # Datasets & Loaders\n    train_ds = TensorDataset(X_num_train_t, X_cat_train_t, y_train_t)\n    val_ds = TensorDataset(X_num_val_t, X_cat_val_t, y_val_t)\n    train_loader = DataLoader(train_ds, batch_size=256, shuffle=True)\n    val_loader = DataLoader(val_ds, batch_size=1024, shuffle=False)\n\n    # Model\n    model = TabularResNetWithEmbedding(\n        num_numerical=X_num.shape[1],\n        cat_unique_counts=cat_unique_counts,\n        embedding_dim=8,\n        hidden_dim=256,\n        n_blocks=3,\n        dropout=0.11,\n        head_dims=[64, 16]\n    ).to(device)\n\n    # Train\n    model, best_rmse = train_model(\n        model,\n        train_loader,\n        val_loader,\n        epochs=300,\n        lr=1e-3,\n        weight_decay=1e-4,\n        patience=20,\n        factor=0.5,\n        min_lr=1e-6\n    )\n\n    # Predict\n    model.eval()\n    with torch.no_grad():\n        val_pred = model(X_num_val_t.to(device), X_cat_val_t.to(device)).cpu().numpy()\n        test_pred = model(X_test_num_t.to(device), X_test_cat_t.to(device)).cpu().numpy()\n\n    oof_predictions[val_idx] = val_pred\n    test_predictions.append(test_pred)\n\n    print(f\"Fold {fold + 1} RMSE: {best_rmse:.5f}\")\n\n# ==============================\n# Final Evaluation & Submission\n# ==============================\noof_rmse = np.sqrt(mean_squared_error(y, oof_predictions))\nprint(\"\\n\" + \"=\" * 50)\nprint(f\"Final OOF RMSE: {oof_rmse:.6f}\")\nprint(\"=\" * 50)\n\noof_df = pd.DataFrame({'id': train_df['id'], TARGET: oof_predictions})\noof_df.to_csv('nn_oof.csv', index=False)\n\nsubmission_df[TARGET] = np.mean(test_predictions, axis=0)\nsubmission_df.to_csv('submission.csv', index=False)\nprint(\"\\nSubmission saved!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T13:19:57.60078Z","iopub.execute_input":"2026-01-04T13:19:57.601108Z"}},"outputs":[],"execution_count":null}]}